{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Logistic Regression model with Pyspark\n",
    "\n",
    "Spark is a powerful, general purpose tool for working with Big Data. Spark transparently handles the distribution of compute tasks across a cluster. This means that operations are fast, but it also allows you to focus on the analysis rather than worry about technical details. You'll learn Logistic Regression/Classifiers. \n",
    "\n",
    "### Index \n",
    "1. Loading the data\n",
    "2. Data Preparation\n",
    "3. Train/test split\n",
    "4. Build a Logistic Regression model\n",
    "5. Evaluate the Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import SparkSession                     # Import the PySpark module\n",
    "from pyspark.sql.functions import round                  # Import the required function\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression # Import the logistic regression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator    # Import the one hot encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('LogisticRegression') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 50000 records.\n",
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n",
      "50000\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
      "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/danae/Documents/pySparkTraining/files/\"\n",
    "\n",
    "# Read data from CSV file\n",
    "flights = spark.read.csv(path + 'flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "# Check column data types\n",
    "print(flights.dtypes)\n",
    "print(flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "removing an uninformative column and\n",
    "removing rows which do not have information about whether or not a flight was delayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47022\n",
      "+---+---+---+-------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+---+----+------+--------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 316| 16.33|      82|   30|\n",
      "|  2| 20|  4|     UA|SFO| 337|  6.17|      82|   -8|\n",
      "|  9| 13|  1|     AA|ORD|1236| 10.33|     195|   -5|\n",
      "|  5|  2|  1|     UA|SFO| 550|  7.98|     102|    2|\n",
      "|  7|  2|  6|     AA|ORD| 733| 10.83|     135|   54|\n",
      "+---+---+---+-------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "flights_drop_column = flights.drop('flight')\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "flights_drop_column.filter('delay IS NULL').count()\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "flights_none_missing = flights_valid_delay.dropna()\n",
    "print(flights_none_missing.count())\n",
    "\n",
    "flights_none_missing.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Column manipulation\n",
    "\n",
    "The Federal Aviation Administration (FAA) considers a flight to be \"delayed\" when it arrives 15 minutes or more after its scheduled time.\n",
    "\n",
    "The next step of preparing the flight data has two parts:\n",
    "\n",
    "1. convert the units of distance, replacing the mile column with a `kmcolumn`; and\n",
    "2. create a Boolean column indicating whether or not a flight was delayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "47022\n"
     ]
    }
   ],
   "source": [
    "# Convert 'mile' to 'km' and drop 'mile' column\n",
    "flights_km = flights_none_missing.withColumn('km', round(flights.mile * 1.60934 , 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights_km = flights_km.withColumn('label', (flights_km.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "flights_km.show(5)\n",
    "print(flights_km.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Categorical columns\n",
    "In the flights data there are two columns, carrier and org, which hold categorical data. You need to transform those columns into indexed numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(flights_km)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "flights_indexed = indexer_model.transform(flights_km)\n",
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|  0| 22|  2|     UA|ORD| 16.33|      82|   30| 509.0|    1|        0.0|    0.0|\n",
      "|  2| 20|  4|     UA|SFO|  6.17|      82|   -8| 542.0|    0|        0.0|    1.0|\n",
      "|  9| 13|  1|     AA|ORD| 10.33|     195|   -5|1989.0|    0|        1.0|    0.0|\n",
      "|  5|  2|  1|     UA|SFO|  7.98|     102|    2| 885.0|    0|        0.0|    1.0|\n",
      "|  7|  2|  6|     AA|ORD| 10.83|     135|   54|1180.0|    1|        1.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx')\\\n",
    "                                .fit(flights_indexed)\\\n",
    "                                .transform(flights_indexed)\n",
    "flights_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to encoding categorical features is to create a `StringIndexer`. Members of this class are **Estimators** that take a DataFrame with a column of strings and map each unique string to a number. \n",
    "\n",
    "Then, the *Estimator* returns a **Transformer** that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|carrier|carrier_idx|\n",
      "+-------+-----------+\n",
      "|     UA|        0.0|\n",
      "|     AA|        1.0|\n",
      "|     OO|        2.0|\n",
      "|     WN|        3.0|\n",
      "|     B6|        4.0|\n",
      "|     OH|        5.0|\n",
      "|     US|        6.0|\n",
      "|     HA|        7.0|\n",
      "|     AQ|        8.0|\n",
      "+-------+-----------+\n",
      "\n",
      "+---+-------+\n",
      "|org|org_idx|\n",
      "+---+-------+\n",
      "|ORD|    0.0|\n",
      "|SFO|    1.0|\n",
      "|JFK|    2.0|\n",
      "|LGA|    3.0|\n",
      "|SMF|    4.0|\n",
      "|SJC|    5.0|\n",
      "|TUS|    6.0|\n",
      "|OGG|    7.0|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_indexed.select('carrier', 'carrier_idx').distinct().orderBy('carrier_idx').show()\n",
    "flights_indexed.select('org', 'org_idx').distinct().orderBy('org_idx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SMF|    4.0|(7,[4],[1.0])|\n",
      "|SJC|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the one hot encoder\n",
    "flights_onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = flights_onehot.fit(flights_indexed)\n",
    "flights_onehot = onehot.transform(flights_indexed)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+\n",
      "|carrier|carrier_idx|carrier_dummy|\n",
      "+-------+-----------+-------------+\n",
      "|     UA|        0.0|(8,[0],[1.0])|\n",
      "|     AA|        1.0|(8,[1],[1.0])|\n",
      "|     OO|        2.0|(8,[2],[1.0])|\n",
      "|     WN|        3.0|(8,[3],[1.0])|\n",
      "|     B6|        4.0|(8,[4],[1.0])|\n",
      "|     OH|        5.0|(8,[5],[1.0])|\n",
      "|     US|        6.0|(8,[6],[1.0])|\n",
      "|     HA|        7.0|(8,[7],[1.0])|\n",
      "|     AQ|        8.0|    (8,[],[])|\n",
      "+-------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Repeat the process for the other categorical feature\n",
    "flights_onehot = OneHotEncoderEstimator(inputCols=['carrier_idx']\n",
    "                                        , outputCols=['carrier_dummy'])\\\n",
    "                .fit(flights_onehot)\\\n",
    "                .transform(flights_onehot)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('carrier', 'carrier_idx', 'carrier_dummy')\\\n",
    "    .distinct()\\\n",
    "    .sort('carrier_idx')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Assembling columns\n",
    "The final stage of data preparation is to consolidate all of the predictor columns into a single column.\n",
    "\n",
    "This has to be done before modeling because every Spark modeling routine expects the data to be in this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------+\n",
      "|label|features                                                            |\n",
      "+-----+--------------------------------------------------------------------+\n",
      "|1    |(21,[1,2,3,11,18,19,20],[22.0,2.0,1.0,1.0,509.0,16.33,82.0])        |\n",
      "|0    |(21,[0,1,2,3,12,18,19,20],[2.0,20.0,4.0,1.0,1.0,542.0,6.17,82.0])   |\n",
      "|0    |(21,[0,1,2,4,11,18,19,20],[9.0,13.0,1.0,1.0,1.0,1989.0,10.33,195.0])|\n",
      "|0    |(21,[0,1,2,3,12,18,19,20],[5.0,2.0,1.0,1.0,1.0,885.0,7.98,102.0])   |\n",
      "|1    |(21,[0,1,2,4,11,18,19,20],[7.0,2.0,6.0,1.0,1.0,1180.0,10.83,135.0]) |\n",
      "+-----+--------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_dummy', 'org_dummy', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_onehot)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('label', 'features').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/test split\n",
    "\n",
    "To objectively assess a Machine Learning model you need to be able to test it on an independent set of data. You can't use the same data that you used to train the model: of course the model will perform (relatively) well on those data!\n",
    "\n",
    "You will split the data into two components:\n",
    "\n",
    "- training data (used to train the model) and\n",
    "- testing data (used to test the model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75054\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = flights_train.count() / flights.count()\n",
    "print(training_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Logistic Regression model\n",
    "The objective is to predict whether a flight is likely to be delayed by at least 15 minutes (label 1) or not (label 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+----------+\n",
      "|label|         probability|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|    1|[0.44630381940577...|       1.0|\n",
      "|    1|[0.36353388740185...|       1.0|\n",
      "|    1|[0.28861696644354...|       1.0|\n",
      "|    1|[0.21152096951843...|       1.0|\n",
      "|    1|[0.24782054530780...|       1.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0| 1582|\n",
      "|    0|       0.0| 2580|\n",
      "|    1|       1.0| 3242|\n",
      "|    0|       1.0| 2091|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(flights_test)\n",
    "\n",
    "prediction.select('label', 'probability', 'prediction').show(5)\n",
    "\n",
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the Logistic Regression model\n",
    "Accuracy is generally not a very reliable metric because it can be biased by the most common target class.\n",
    "\n",
    "There are two other useful metrics : precision and recall.\n",
    "\n",
    "- **Precision** is the proportion of positive predictions which are correct. For all flights which are predicted to be delayed, what proportion is actually delayed?\n",
    "\n",
    "- **Recall** is the proportion of positives outcomes which are correctly predicted. For all delayed flights, what proportion is correctly predicted by the model?\n",
    "\n",
    "The precision and recall are generally formulated in terms of the positive target class. But it's also possible to calculate weighted versions of these metrics which look at both target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6131648235913639\n"
     ]
    }
   ],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label != prediction').count()\n",
    "FP = prediction.filter('prediction = 1 AND label != prediction').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TN+TP)/(TN+TP+FN+FP)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.61\n",
      "recall    = 0.67\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator =  MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \n",
    "                                                           'weightedPrecision'})\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName:'areaUnderROC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the cluster\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
