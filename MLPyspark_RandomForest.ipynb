{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a  Gradient-Boosted Trees with PySpark\n",
    "\n",
    "You've previously built a classifier for flights likely to be delayed using a Decision Tree. In this exercise you'll compare a Decision Tree model to a Gradient-Boosted Trees model.\n",
    "\n",
    "## 1. Import the classes required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql import SparkSession            # Import the PySpark module\n",
    "from pyspark.sql.functions import round         # Import the required function\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler  # Import the necessary class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier # Decision Tree Classifier \n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading flights data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/danae/Documents/pySparkTraining/files/\"\n",
    "# Read data from CSV file\n",
    "flights = spark.read.csv(path + 'flights.csv',\n",
    "                         sep = ',',\n",
    "                         header = True,\n",
    "                         inferSchema = True,\n",
    "                         nullValue = 'NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "\n",
    "You previously loaded airline flight data from a CSV file. You're going to develop a model which will predict whether or not a given flight will be delayed.\n",
    "\n",
    "In this exercise you need to trim those data down by:\n",
    "\n",
    "removing an uninformative column and\n",
    "removing rows which do not have information about whether or not a flight was delayed.\n",
    "\n",
    "### 3.1 Column manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47022\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+------+-----+\n",
      "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30| 509.0|    1|\n",
      "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8| 542.0|    0|\n",
      "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|1989.0|    0|\n",
      "|  5|  2|  1|     UA|   704|SFO| 550|  7.98|     102|    2| 885.0|    0|\n",
      "|  7|  2|  6|     AA|   380|ORD| 733| 10.83|     135|   54|1180.0|    1|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flights = flights.filter('delay IS NOT NULL').dropna()\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column\n",
    "flights = flights.withColumn('km', round(flights.mile * 1.60934 , 0))\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "flights = flights.withColumn('label', (flights.delay >= 15).cast('integer'))\n",
    "\n",
    "# number of rows\n",
    "print(flights.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical columns\n",
    "In the flights data there are two columns, carrier and org, which hold categorical data. You need to transform those columns into indexed numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f913d322f80>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/danae/spark/python/pyspark/ml/wrapper.py\", line 40, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'StringIndexer' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "# Create an indexer\n",
    "flights_indexed = StringIndexer(inputCol = 'carrier', outputCol = 'carrier_idx')\\\n",
    "                                .fit(flights)\\\n",
    "                                .transform(flights)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx')\\\n",
    "                                .fit(flights_indexed)\\\n",
    "                                .transform(flights_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to encoding categorical features is to create a `StringIndexer`. Members of this class are **Estimators** that take a DataFrame with a column of strings and map each unique string to a number. \n",
    "\n",
    "Then, the *Estimator* returns a **Transformer** that takes a DataFrame, attaches the mapping to it as metadata, and returns a new DataFrame with a numeric column corresponding to the string column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---+-------+\n",
      "|carrier|carrier_idx|org|org_idx|\n",
      "+-------+-----------+---+-------+\n",
      "|     UA|        0.0|ORD|    0.0|\n",
      "|     UA|        0.0|SFO|    1.0|\n",
      "|     UA|        0.0|JFK|    2.0|\n",
      "|     UA|        0.0|LGA|    3.0|\n",
      "|     UA|        0.0|SMF|    4.0|\n",
      "+-------+-----------+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_indexed.select('carrier', 'carrier_idx', 'org', 'org_idx')\\\n",
    "                .distinct()\\\n",
    "                .orderBy('carrier_idx', 'org_idx').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Assembling columns\n",
    "The final stage of data preparation is to consolidate all of the predictor columns into a single column.\n",
    "\n",
    "This has to be done before modeling because every Spark modeling routine expects the data to be in this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|features                                 |label|\n",
      "+-----------------------------------------+-----+\n",
      "|[0.0,22.0,2.0,0.0,0.0,509.0,16.33,82.0]  |1    |\n",
      "|[2.0,20.0,4.0,0.0,1.0,542.0,6.17,82.0]   |0    |\n",
      "|[9.0,13.0,1.0,1.0,0.0,1989.0,10.33,195.0]|0    |\n",
      "|[5.0,2.0,1.0,0.0,1.0,885.0,7.98,102.0]   |0    |\n",
      "|[7.0,2.0,6.0,1.0,0.0,1180.0,10.83,135.0] |1    |\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_mod = flights_indexed.select('mon', 'dom', 'dow', 'carrier_idx', \n",
    "                         'org_idx', 'km', 'depart', 'duration', 'delay', 'label')\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "flights_assembled = assembler.transform(flights_mod)\n",
    "\n",
    "# Check the resulting column\n",
    "flights_assembled.select('features','label').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/test split\n",
    "\n",
    "To objectively assess a Machine Learning model you need to be able to test it on an independent set of data. You can't use the same data that you used to train the model: of course the model will perform (relatively) well on those data!\n",
    "\n",
    "You will split the data into two components:\n",
    "\n",
    "- training data (used to train the model) and\n",
    "- testing data (used to test the model).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "flights_train, flights_test = flights_assembled.randomSplit([0.8, 0.2], seed=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build the models \n",
    "Now that you've split the flights data into training and testing sets, you can use the training set to fit a Decision Tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "gbt = GBTClassifier()\n",
    "\n",
    "tree_model = tree.fit(flights_train)\n",
    "gbt_model = gbt.fit(flights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7198696758978518"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare AUC on testing data\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Create predictions for the testing data \n",
    "prediction_tree = tree_model.transform(flights_test)\n",
    "prediction_gbt = gbt_model.transform(flights_test)\n",
    "\n",
    "evaluator.evaluate(prediction_tree)\n",
    "evaluator.evaluate(prediction_gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,[0,1,2,3,4,5,6,7],[0.18206047719666724,0.16061942914144267,0.1425136430435656,0.09221722599904801,0.16716807895229843,0.06189958003724091,0.13495097255514493,0.05857059307459232])\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Find the number of trees and the relative importance of features\n",
    "print(gbt_model.featureImportances)\n",
    "print(gbt_model.getNumTrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------------------------------------+\n",
      "|label|prediction|probability                             |\n",
      "+-----+----------+----------------------------------------+\n",
      "|1    |1.0       |[0.38793503480278424,0.6120649651972158]|\n",
      "|1    |1.0       |[0.38793503480278424,0.6120649651972158]|\n",
      "|0    |1.0       |[0.3168674698795181,0.6831325301204819] |\n",
      "|1    |1.0       |[0.38793503480278424,0.6120649651972158]|\n",
      "|1    |1.0       |[0.38793503480278424,0.6120649651972158]|\n",
      "+-----+----------+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed flights with a Random Forest\n",
    "\n",
    "In this exercise you'll bring together cross validation and ensemble methods. You'll be training a Random Forest classifier to predict delayed flights, using cross validation to choose the best values for model parameters.\n",
    "\n",
    "You'll find good values for the following parameters:\n",
    "\n",
    "- `featureSubsetStrategy` — the number of features to consider for splitting at each node and\n",
    "- `maxDepth` — the maximum number of splits along any branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "# Create a random forest classifier\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "# Create a parameter grid\n",
    "params = ParamGridBuilder() \\\n",
    "            .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\n",
    "            .addGrid(forest.maxDepth, [2, 5, 10]) \\\n",
    "            .build()\n",
    "\n",
    "# Create a binary classification evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Create a cross-validator\n",
    "cv = CrossValidator(estimator = forest\n",
    "                    , estimatorParamMaps = params\n",
    "                    , evaluator = evaluator\n",
    "                    , numFolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average AUC for each parameter combination in grid\n",
    "avg_auc = cv.avgMetrics\n",
    "\n",
    "# Average AUC for the best model\n",
    "best_model_auc =  max(avg_auc)\n",
    "\n",
    "# What's the optimal parameter value?\n",
    "opt_max_depth = cv.bestModel.explainParam('maxDepth')\n",
    "opt_feat_substrat = cv.bestModel.explainParam('featureSubsetStrategy')\n",
    "\n",
    "# AUC for best model on testing data\n",
    "best_auc = evaluator.evaluate(cv.transform(flights_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the cluster\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
