{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark\n",
    "\n",
    "### Index \n",
    "\n",
    "## Introduction\n",
    "\n",
    "1. What is Spark?\n",
    "2. Using Spark in Python\n",
    "    - 2.1 Connecting to a cluster\n",
    "3. Examining the SparkContext\n",
    "4. Using DataFrames\n",
    "    - 4.1 Creating a SparkSession\n",
    "5. Creating a DataFrame from a local file \n",
    "6. Creating a DataFrame from a RDD \n",
    "\n",
    "## Manipulating data\n",
    "\n",
    "7. Creating columns\n",
    "8. Renaming columns\n",
    "9. Selecting columns \n",
    "10. Creating columns with the .select() method\n",
    "11. Filtering\n",
    "12. Joins \n",
    "13. Aggregating\n",
    "\n",
    "## Using SQL \n",
    "\n",
    "14. SQL reminder\n",
    "15. Pandafy a Spark DataFrame\n",
    "16. Put some Spark in your data\n",
    "17. Viewing tables\n",
    "\n",
    "## 1. What is Spark?\n",
    "\n",
    "Spark is a platform for cluster computing. Spark lets you spread data and computations over clusters with multiple nodes (think of each node as a separate computer). Splitting up your data makes it easier to work with very large datasets because each node only works with a small amount of data.\n",
    "\n",
    "As each node works on its own subset of the total data, it also carries out a part of the total calculations required, so that both data processing and computation are performed in parallel over the nodes in the cluster. It is a fact that parallel computation can make certain types of programming tasks much faster.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Using Spark in Python\n",
    "\n",
    "### 2.1 Connecting to a cluster\n",
    "\n",
    "The first step in using Spark is connecting to a cluster. In practice, the cluster will be hosted on a remote machine that's connected to all other nodes. There will be one computer, called the **master** (or driver) that manages splitting up the data and the computations. The master is connected to the rest of the computers in the cluster, which are called **workers**. The master sends the workers data and calculations to run, and they send their results back to the master."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField \n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Examining The SparkContext\n",
    "\n",
    "Creating the connection is as simple as creating an instance of the `SparkContext` class. The class constructor takes a few optional arguments that allow you to specify the attributes of the cluster you're connecting to. Take a look at the [documentation](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html) for all the details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(master = \"local\", appName = \"Introduction to pySpark\") \n",
    "\n",
    "# Verify SparkContext\n",
    "print(sc)\n",
    "# Print Spark version\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll probably notice that code takes longer to run than you might expect. This is because Spark is some serious software. It takes more time to start up than you might be used to. You may also find that running simpler computations might take longer than expected. That's because all the optimizations that Spark has under its hood are designed for complicated operations with big data sets. That means that for simple or small problems Spark may actually perform worse than some other solutions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using DataFrames\n",
    "\n",
    "Spark's core data structure is the **Resilient Distributed Dataset** (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so we'll be using the Spark `DataFrame` abstraction built on top of RDDs.\n",
    "\n",
    "The Spark `DataFrame` was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows). Not only are they easier to understand, `DataFrames` are also more optimized for complicated operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using `RDD`s, it's up to the data scientist to figure out the right way to optimize the query, but the `DataFrame` implementation has much of this optimization built in!\n",
    "\n",
    "### 4.1 Creating a SparkSession\n",
    "\n",
    "To start working with Spark `DataFrames`, you first have to create a `SparkSession` object from your `SparkContext`. You can think of the `SparkContext` as your connection to the cluster and the `SparkSession` as your interface with that connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AA_DFW_2014_Departures_Short.csv  DallasCouncilVotes.csv  juegos.csv\r\n",
      "AA_DFW_2015_Departures_Short.csv  deporte.csv\t\t  modelo_relacional.jpg\r\n",
      "AA_DFW_2016_Departures_Short.csv  deportista2.csv\t  paises.csv\r\n",
      "AA_DFW_2017_Departures_Short.csv  deportista.csv\t  people.csv\r\n",
      "cheatsheet_spark.pdf\t\t  deportistaError.csv\t  resultados.csv\r\n",
      "DallasCouncilVoters.csv\t\t  evento.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Files we will be working with\n",
    "path = \"/home/danae/Documents/pySparkTraining/files/\"\n",
    "\n",
    "!ls /home/danae/Documents/pySparkTraining/files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already created a `SparkSession` called `spark`, but what if you're not sure there already is one? Creating multiple `SparkSession`s and `SparkContext`s can cause issues, so it's best practice to use the `SparkSession.builder.getOrCreate()` method. This returns an existing `SparkSession` if there's already one in the environment, or creates a new one if necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7ff80073b190>\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Print spark\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating a DataFrame from a local file \n",
    "\n",
    "Your SparkSession has a `.read` attribute which has several methods for reading different data sources into Spark `DataFrames`. Using these you can create a DataFrame from a `.csv` file just like with regular pandas DataFrames!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|deporte_id|      deporte|\n",
      "+----------+-------------+\n",
      "|         1|   Basketball|\n",
      "|         2|         Judo|\n",
      "|         3|     Football|\n",
      "|         4|   Tug-Of-War|\n",
      "|         5|Speed Skating|\n",
      "+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sport = spark.read.csv(path + 'deporte.csv', header = True)\n",
    "\n",
    "# Show the data\n",
    "sport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- deporte_id: string (nullable = true)\n",
      " |-- deporte: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sport.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can achive the same results and also specify a schema with the `sqlContext.read.schema` function in order to specify the type of column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|sport_id|        sport|\n",
      "+--------+-------------+\n",
      "|       1|   Basketball|\n",
      "|       2|         Judo|\n",
      "|       3|     Football|\n",
      "|       4|   Tug-Of-War|\n",
      "|       5|Speed Skating|\n",
      "+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sportSchema = StructType([\n",
    "    StructField('sport_id', IntegerType(), False),\n",
    "    StructField('sport', StringType(), False)\n",
    "])\n",
    "\n",
    "sportDF = sqlContext.read.schema(sportSchema) \\\n",
    "            .option('header', 'true').csv(path + 'deporte.csv')\n",
    "\n",
    "sportDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+\n",
      "|event_id|                name|sport_id|\n",
      "+--------+--------------------+--------+\n",
      "|       1|Basketball Men's ...|       1|\n",
      "|       2|Judo Men's Extra-...|       2|\n",
      "|       3|Football Men's Fo...|       3|\n",
      "|       4|Tug-Of-War Men's ...|       4|\n",
      "|       5|Speed Skating Wom...|       5|\n",
      "+--------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eventSchema = StructType([\n",
    "    StructField('event_id', IntegerType(), False),\n",
    "    StructField('name', StringType(), False),\n",
    "    StructField('sport_id', IntegerType(), False),    \n",
    "])\n",
    "\n",
    "OlimpicSportsDF = sqlContext.read.schema(eventSchema) \\\n",
    "            .option('header', 'true').csv(path + 'evento.csv')\n",
    "\n",
    "OlimpicSportsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----+------+---------+\n",
      "|game_id|  name_game|year|season|     city|\n",
      "+-------+-----------+----+------+---------+\n",
      "|      1|1896 Verano|1896|Verano|   Athina|\n",
      "|      2|1900 Verano|1900|Verano|    Paris|\n",
      "|      3|1904 Verano|1904|Verano|St. Louis|\n",
      "|      4|1906 Verano|1906|Verano|   Athina|\n",
      "|      5|1908 Verano|1908|Verano|   London|\n",
      "+-------+-----------+----+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gameSchema = StructType([\n",
    "    StructField('game_id', IntegerType(), False),\n",
    "    StructField('name_game', StringType(), False),\n",
    "    StructField('year', StringType(), False),\n",
    "    StructField('season', StringType(), False),\n",
    "    StructField('city', StringType(), False)\n",
    "])\n",
    "\n",
    "gameDF = sqlContext.read.schema(gameSchema) \\\n",
    "            .option('header', 'true').csv(path + 'juegos.csv')\n",
    "\n",
    "gameDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+\n",
      "| id|                team|country|\n",
      "+---+--------------------+-------+\n",
      "|  1|         30. Februar|    AUT|\n",
      "|  2|A North American ...|    MEX|\n",
      "|  3|           Acipactli|    MEX|\n",
      "|  4|             Acturus|    ARG|\n",
      "|  5|         Afghanistan|    AFG|\n",
      "+---+--------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teamSchema = StructType([\n",
    "    StructField('id', IntegerType(), False),\n",
    "    StructField('team', StringType(), False),\n",
    "    StructField('country', StringType(), False),\n",
    "])\n",
    "\n",
    "countryDF = sqlContext.read.schema(teamSchema) \\\n",
    "            .option('header', 'true').csv(path + 'paises.csv')\n",
    "\n",
    "countryDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+-------+--------+\n",
      "|result_id|medal|athlete_id|game_id|event_id|\n",
      "+---------+-----+----------+-------+--------+\n",
      "|        1|   NA|         1|     39|       1|\n",
      "|        2|   NA|         2|     49|       2|\n",
      "|        3|   NA|         3|      7|       3|\n",
      "|        4| Gold|         4|      2|       4|\n",
      "|        5|   NA|         5|     36|       5|\n",
      "+---------+-----+----------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultSchema = StructType([\n",
    "    StructField('result_id', IntegerType(), False),\n",
    "    StructField('medal', StringType(), False),\n",
    "    StructField('athlete_id', IntegerType(), False),\n",
    "    StructField('game_id', IntegerType(), False),\n",
    "    StructField('event_id', IntegerType(), False),    \n",
    "])\n",
    "\n",
    "resultDF = sqlContext.read.schema(resultSchema) \\\n",
    "            .option('header', 'true').csv(path + 'resultados.csv')\n",
    "\n",
    "resultDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating a DataFrame from a RDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deportista_id', 'nombre', 'genero', 'edad', 'altura', 'peso', 'equipo_id'],\n",
       " ['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0', '273'],\n",
       " ['4', 'Edgar Lindenau Aabye', '1', '34', '0', '0', '278']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athleteRDD = sc.textFile(path +'deportista.csv') \\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "athleteRDD2 = sc.textFile(path + 'deportista2.csv') \\\n",
    "    .map(lambda line: line.split(\",\"))\n",
    "\n",
    "athleteRDD = athleteRDD.union(athleteRDD2) # operations using RDDs\n",
    "athleteRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135572"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athleteRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'A Dijiang', '1', '24', '180', '80', '199'],\n",
       " ['2', 'A Lamusi', '1', '23', '170', '60', '199'],\n",
       " ['3', 'Gunnar Nielsen Aaby', '1', '24', '0', '0', '273'],\n",
       " ['4', 'Edgar Lindenau Aabye', '1', '34', '0', '0', '278'],\n",
       " ['5', 'Christine Jacoba Aaftink', '2', '21', '185', '82', '705']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def deleteHeadline(index, iterator):\n",
    "    return iter(list(iterator)[1:])\n",
    "\n",
    "athleteRDD = athleteRDD.mapPartitionsWithIndex(deleteHeadline)\n",
    "athleteRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'A Dijiang', 1, 24, 180, 80.0, 199),\n",
       " (2, 'A Lamusi', 1, 23, 170, 60.0, 199),\n",
       " (3, 'Gunnar Nielsen Aaby', 1, 24, 0, 0.0, 273),\n",
       " (4, 'Edgar Lindenau Aabye', 1, 34, 0, 0.0, 278),\n",
       " (5, 'Christine Jacoba Aaftink', 2, 21, 185, 82.0, 705)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athleteRDD = athleteRDD.map(lambda l: (int(l[0]),l[1],int(l[2]),int(l[3]),int(l[4]),float(l[5]),int(l[6])))\n",
    "athleteRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+---+------+------+-------+\n",
      "|athlete_id|                name|gender|age|height|weigth|team_id|\n",
      "+----------+--------------------+------+---+------+------+-------+\n",
      "|         1|           A Dijiang|     1| 24|   180|  80.0|    199|\n",
      "|         2|            A Lamusi|     1| 23|   170|  60.0|    199|\n",
      "|         3| Gunnar Nielsen Aaby|     1| 24|     0|   0.0|    273|\n",
      "|         4|Edgar Lindenau Aabye|     1| 34|     0|   0.0|    278|\n",
      "|         5|Christine Jacoba ...|     2| 21|   185|  82.0|    705|\n",
      "+----------+--------------------+------+---+------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a DataFrame from the RDD \n",
    "schema = StructType([\n",
    "    StructField('athlete_id', IntegerType(), False),\n",
    "    StructField('name', StringType(), False),\n",
    "    StructField('gender', IntegerType(), False),\n",
    "    StructField('age', IntegerType(), False),\n",
    "    StructField('height', IntegerType(), False),\n",
    "    StructField('weigth', FloatType(), False),\n",
    "    StructField('team_id', IntegerType(), False)\n",
    "])\n",
    "\n",
    "athleteDF = sqlContext.createDataFrame(athleteRDD, schema)\n",
    "athleteDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulating data\n",
    "\n",
    "### Index \n",
    "7. Creating columns\n",
    "8. Renaming columns\n",
    "9. Selecting columns \n",
    "10. Creating columns with the .select() method\n",
    "11. Filtering\n",
    "12. Joins \n",
    "13. Aggregating\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Creating columns\n",
    "\n",
    "Let's explore the methods defined by Spark's `DataFrame` class to perform common data operations.\n",
    "\n",
    "For performing column-wise operations in Spark you can use the **`.withColumn()`** method, which takes two arguments. First, a string with the name of your new column, and second the new column itself.\n",
    "\n",
    "The new column must be an object of class `Column`. Creating one of these is as easy as extracting a column from your `DataFrame` using the syntax `df.colName`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+---+------+------+-------+-------------+\n",
      "|athlete_id|                name|gender|age|height|weigth|team_id|weigth_pounds|\n",
      "+----------+--------------------+------+---+------+------+-------+-------------+\n",
      "|         1|           A Dijiang|     1| 24|   180|  80.0|    199|        176.0|\n",
      "|         2|            A Lamusi|     1| 23|   170|  60.0|    199|        132.0|\n",
      "|         3| Gunnar Nielsen Aaby|     1| 24|     0|   0.0|    273|          0.0|\n",
      "|         4|Edgar Lindenau Aabye|     1| 34|     0|   0.0|    278|          0.0|\n",
      "|         5|Christine Jacoba ...|     2| 21|   185|  82.0|    705|        180.4|\n",
      "+----------+--------------------+------+---+------+------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "athleteDF = athleteDF.withColumn(\"weigth_pounds\", athleteDF.weigth*2.2)\n",
    "athleteDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code creates a `DataFrame` with the same columns as `athleteDF` plus a new column, `weigth_pounds`, where every entry is equal to the corresponding entry from `weigth`, multiplied by 2.2\n",
    "\n",
    "Updating a Spark `DataFrame` is somewhat different than working in pandas because the Spark `DataFrame` is immutable. This means that it can't be changed, and so columns can't be updated in place.\n",
    "\n",
    "Thus, all these methods return a new `DataFrame`. To overwrite the original `DataFrame` you must reassign the returned `DataFrame` using the method like so:\n",
    "\n",
    "`df = df.withColumn(\"newCol\", df.oldCol + 1)`\n",
    "\n",
    "--- \n",
    "\n",
    "## 8. Renaming columns\n",
    "\n",
    "To rename columns you can use the **`.withColumnRenamed()`** method, which takes two arguments, the name of the column we want to replace and the new column name we want to asign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---+---+------+------+-------+-------------+\n",
      "|athlete_id|                name|sex|age|height|weigth|team_id|weigth_pounds|\n",
      "+----------+--------------------+---+---+------+------+-------+-------------+\n",
      "|         1|           A Dijiang|  1| 24|   180|  80.0|    199|        176.0|\n",
      "|         2|            A Lamusi|  1| 23|   170|  60.0|    199|        132.0|\n",
      "|         3| Gunnar Nielsen Aaby|  1| 24|     0|   0.0|    273|          0.0|\n",
      "|         4|Edgar Lindenau Aabye|  1| 34|     0|   0.0|    278|          0.0|\n",
      "|         5|Christine Jacoba ...|  2| 21|   185|  82.0|    705|        180.4|\n",
      "+----------+--------------------+---+---+------+------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# renombrado de columnas\n",
    "athleteDF = athleteDF.withColumnRenamed(\"gender\", 'sex')\n",
    "athleteDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Selecting columns \n",
    "\n",
    "The `.select` method takes multiple arguments - one for each column you want to select. These arguments can either be the column name as a **string** (one for each column) or a column object (using the `df.colName` syntax). \n",
    "\n",
    "When you pass a column object, you can perform operations like addition or subtraction on the column to change the data contained in it, much like inside `.withColumn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- athlete_id: integer (nullable = false)\n",
      " |-- name: string (nullable = false)\n",
      " |-- sex: integer (nullable = false)\n",
      " |-- age: integer (nullable = false)\n",
      " |-- height: integer (nullable = false)\n",
      " |-- weigth: float (nullable = false)\n",
      " |-- team_id: integer (nullable = false)\n",
      " |-- weigth_pounds: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "athleteDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---+\n",
      "|athlete_id|                name|age|\n",
      "+----------+--------------------+---+\n",
      "|         1|           A Dijiang| 24|\n",
      "|         2|            A Lamusi| 23|\n",
      "|         3| Gunnar Nielsen Aaby| 24|\n",
      "|         4|Edgar Lindenau Aabye| 34|\n",
      "|         5|Christine Jacoba ...| 21|\n",
      "+----------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "athleteDF2 = athleteDF.select(athleteDF.athlete_id\n",
    "                              , athleteDF.name\n",
    "                              , athleteDF.age\n",
    "                              )\n",
    "athleteDF2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---+\n",
      "|athlete_id|                name|age|\n",
      "+----------+--------------------+---+\n",
      "|         1|           A Dijiang| 24|\n",
      "|         2|            A Lamusi| 23|\n",
      "|         3| Gunnar Nielsen Aaby| 24|\n",
      "|         4|Edgar Lindenau Aabye| 34|\n",
      "|         5|Christine Jacoba ...| 21|\n",
      "+----------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "athleteDF3 = athleteDF.select(\"athlete_id\"\n",
    "                              , \"name\"\n",
    "                              , \"age\"\n",
    "                              )\n",
    "athleteDF3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between `.select()` and `.withColumn()` methods is that `.select()` returns only the columns you specify, while `.withColumn()` returns all the columns of the DataFrame in addition to the one you defined. \n",
    "\n",
    "--- \n",
    "\n",
    "It's often a good idea to drop columns you don't need at the beginning of an operation so that you're not dragging around extra data as you're wrangling. In this case, you would use `.select()` and not `.withColumn()`.\n",
    "\n",
    "--- \n",
    "\n",
    "## 10. Creating columns with the .select() method\n",
    "\n",
    "Similar to SQL, you can also use the `.select()` method to perform column-wise operations. When you're selecting a column using the `df.colName` notation, you can perform any column operation and the `.select()` method will return the transformed column. \n",
    "\n",
    "You can also use the `.alias()` method to rename a column you're selecting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n",
      "|height|weigth|                BMI|\n",
      "+------+------+-------------------+\n",
      "|   180|  80.0|0.19753086419753085|\n",
      "|   170|  60.0|0.12456747404844293|\n",
      "|     0|   0.0|               null|\n",
      "|     0|   0.0|               null|\n",
      "|   185|  82.0|0.19646457268078893|\n",
      "+------+------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "athleteBMI = athleteDF.select(athleteDF.height\n",
    "                            , athleteDF.weigth\n",
    "                            , ((athleteDF.weigth/athleteDF.height)**2).alias(\"BMI\"))\n",
    "athleteBMI.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equivalent Spark `DataFrame` method `.selectExpr()` takes SQL expressions as a string with the SQL `as` keyword being equivalent to the `.alias()` method. To select multiple columns, you can pass multiple strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                BMI|\n",
      "+-------------------+\n",
      "|0.19753086419753085|\n",
      "|0.12456747404844293|\n",
      "|               null|\n",
      "|               null|\n",
      "|0.19646457268078893|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BMI = athleteDF.selectExpr(\"power(weigth/height,2) as BMI\")\n",
    "BMI.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-------+\n",
      "|athlete_id|                name|age_in_game|team_id|\n",
      "+----------+--------------------+-----------+-------+\n",
      "|         1|           A Dijiang|         24|    199|\n",
      "|         2|            A Lamusi|         23|    199|\n",
      "|         3| Gunnar Nielsen Aaby|         24|    273|\n",
      "|         4|Edgar Lindenau Aabye|         34|    278|\n",
      "|         5|Christine Jacoba ...|         21|    705|\n",
      "+----------+--------------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_athleteDF = athleteDF.select(\"athlete_id\"\n",
    "                             , \"name\"\n",
    "                             , col(\"age\").alias(\"age_in_game\")\n",
    "                             ,\"team_id\")\n",
    "sub_athleteDF.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Filtering\n",
    "\n",
    "The `.filter()` method takes a Spark Column combined with a logical expression generating boolean (True/False) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-------+\n",
      "|athlete_id|                name|age_in_game|team_id|\n",
      "+----------+--------------------+-----------+-------+\n",
      "|         1|           A Dijiang|         24|    199|\n",
      "|         2|            A Lamusi|         23|    199|\n",
      "|         3| Gunnar Nielsen Aaby|         24|    273|\n",
      "|         4|Edgar Lindenau Aabye|         34|    278|\n",
      "|         5|Christine Jacoba ...|         21|    705|\n",
      "+----------+--------------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_athleteDF2 = sub_athleteDF.filter(sub_athleteDF.age_in_game != 0)\n",
    "sub_athleteDF2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's `.filter()` can accept any expression that could go in the `WHERE` clause of a SQL query, **as long as it is passed as a string**. Notice that in this case, we do not reference the name of the table in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-------+\n",
      "|athlete_id|                name|age_in_game|team_id|\n",
      "+----------+--------------------+-----------+-------+\n",
      "|         1|           A Dijiang|         24|    199|\n",
      "|         2|            A Lamusi|         23|    199|\n",
      "|         3| Gunnar Nielsen Aaby|         24|    273|\n",
      "|         4|Edgar Lindenau Aabye|         34|    278|\n",
      "|         5|Christine Jacoba ...|         21|    705|\n",
      "+----------+--------------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_athleteDF3 = sub_athleteDF.filter(\"age_in_game <> 0\")\n",
    "sub_athleteDF3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+-------+\n",
      "|athlete_id|                name|age_in_game|team_id|\n",
      "+----------+--------------------+-----------+-------+\n",
      "|     71691|  Dimitrios Loundras|         10|    333|\n",
      "|     52070|        Etsuko Inada|         11|    514|\n",
      "|     40129|    Luigina Giavotti|         11|    507|\n",
      "|     37333|Carlos Bienvenido...|         11|    982|\n",
      "|     47618|Sonja Henie Toppi...|         11|    742|\n",
      "+----------+--------------------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sub_athleteDF2.sort(\"age_in_game\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Joins \n",
    "\n",
    "A join will combine two different tables along a column that they share. This column is called the key.\n",
    "\n",
    "In PySpark, joins are performed using the DataFrame method `.join()`. This method takes three arguments. The first is the second DataFrame that you want to join with the first one. The second argument, `on`, is the name of the key column(s) as a string, (but for using this syntax, the names of the key column(s) must be the same in each table). The third argument, `how`, specifies the kind of join to perform.\n",
    "\n",
    "If the tables you want to join have different key names, you can use the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------+----+-----+\n",
      "|country|               name|     sport|year|medal|\n",
      "+-------+-------------------+----------+----+-----+\n",
      "|    CHN|          A Dijiang|Basketball|1992|   NA|\n",
      "|    CHN|           A Lamusi|      Judo|2012|   NA|\n",
      "|    DEN|Gunnar Nielsen Aaby|  Football|1920|   NA|\n",
      "+-------+-------------------+----------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medal_per_year = athleteDF.join(resultDF, athleteDF.athlete_id == resultDF.athlete_id, \"left\") \\\n",
    "                          .join(gameDF, gameDF.game_id == resultDF.game_id,  \"left\") \\\n",
    "                          .join(countryDF, athleteDF.team_id == countryDF.id, \"left\") \\\n",
    "                          .join(sportDF, sportDF.sport_id == resultDF.event_id, \"left\") \\\n",
    "                          .select(\"country\", \"name\", \"sport\", \"year\", \"medal\")\n",
    "    \n",
    "medal_per_year.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Aggregating\n",
    "\n",
    "All of the common aggregation methods, like `.min()`, `.max()`, and `.count()` are `GroupedData` methods. These are created by calling the `.groupBy()` DataFrame method. \n",
    "\n",
    "For example, to find the minimum value of a column, `col`, in a DataFrame, `df`, you could do\n",
    "\n",
    "`df.groupBy().min(\"col\").show()`\n",
    "\n",
    "This creates a `GroupedData` object (so you can use the `.min()` method), then finds the minimum value in `col`, and returns it as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(age)|\n",
      "+------------------+\n",
      "|23.299586929261636|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "athleteDF.groupBy().avg(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to achive this, is by using the `.agg()` method with a dictionary indicating the column and the function (any of the aggregate functions from the `pyspark.sql.functions` submodule) you want to apply. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.299586929261636"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_age = athleteDF.agg({'age': 'mean'}).collect()[0][0]\n",
    "mean_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you pass the name of one or more columns in your DataFrame to the `.groupBy()` method, the aggregation methods behave like when you use a `GROUP BY` statement in a SQL query!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------------+-----+\n",
      "|country|year|           sport|count|\n",
      "+-------+----+----------------+-----+\n",
      "|    USA|1896|         Curling|    1|\n",
      "|    FRA|1896|            Polo|    2|\n",
      "|    USA|1896|       Triathlon|    3|\n",
      "|    GRE|1896|Beach Volleyball|    2|\n",
      "|    AUS|1896|         Curling|    1|\n",
      "+-------+----+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another more complex example\n",
    "# remove athets without medals \n",
    "medal_per_year2 = medal_per_year.filter(medal_per_year.medal != \"NA\") \\\n",
    "                    .sort(\"year\") \\\n",
    "                    .groupBy(\"country\", \"year\", \"sport\") \\\n",
    "                    .count()\n",
    "\n",
    "medal_per_year2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|       295|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medal_per_year2.filter(medal_per_year2.country == \"USA\").groupBy().max(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|        77|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medal_per_year2.filter(medal_per_year2.country == \"CAN\").groupBy().max(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|        16|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medal_per_year2.filter(medal_per_year2.country == \"MEX\").groupBy().max(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+------------------+\n",
      "|country|year|Total|           Average|\n",
      "+-------+----+-----+------------------+\n",
      "|    USA|1896|   20|               4.0|\n",
      "|    FRA|1896|   11|3.6666666666666665|\n",
      "|    GRE|1896|   48|               9.6|\n",
      "|    AUS|1896|    3|               1.5|\n",
      "|    GER|1896|   32| 5.333333333333333|\n",
      "+-------+----+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "medal_per_year2.groupBy(\"country\", \"year\")\\\n",
    "        .agg(sum(\"count\").alias(\"Total\"), avg(\"count\").alias(\"Average\"))\\\n",
    "        .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SQL \n",
    "\n",
    "### Index \n",
    "14. SQL reminder\n",
    "15. Pandafy a Spark DataFrame\n",
    "16. Put some Spark in your data\n",
    "17. Viewing tables\n",
    "\n",
    "One of the advantages of the `DataFrame` interface is that you can run SQL queries on the tables in your Spark cluster. \n",
    "\n",
    "Running a query on a table is as easy as using the `.sql()` method on your `SparkSession`. This method takes a string containing the query and returns a `DataFrame` with the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+\n",
      "| id|                team|country|\n",
      "+---+--------------------+-------+\n",
      "|  1|         30. Februar|    AUT|\n",
      "|  2|A North American ...|    MEX|\n",
      "|  3|           Acipactli|    MEX|\n",
      "|  4|             Acturus|    ARG|\n",
      "|  5|         Afghanistan|    AFG|\n",
      "+---+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assign an alias so that it can be used with SQL \n",
    "countryDF.registerTempTable(\"country\") \n",
    "\n",
    "# write your query as a string \n",
    "query = \"FROM country SELECT * LIMIT 5\"\n",
    "\n",
    "# Get the first 5 rows of countryDF\n",
    "country5 = sqlContext.sql(query)\n",
    "\n",
    "# Show the results\n",
    "country5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. SQL reminder\n",
    "\n",
    "A SQL query returns a table derived from one or more tables contained in a database. The two commands that every query has to contain are `SELECT` and `FROM`.The minimal SQL query is:\n",
    "\n",
    "`SELECT * FROM my_table;`\n",
    "\n",
    "The `*` selects all columns, so this returns the entire table named `my_table`. Similar to `.withColumn()`, you can do column-wise computations within a `SELECT` statement. For example,\n",
    "\n",
    "`SELECT weigth, team_id, weigth*2.2 FROM athletesDF `\n",
    "\n",
    "returns a table with the `weigth`, `team_id` and weigth in pounds each athlete. Another commonly used command is `WHERE`. This command filters the rows of the table based on some logical condition you specify. The resulting table contains the rows where your condition is true. For example, if you had a table of students and grades you could do:\n",
    "\n",
    "`SELECT * FROM students WHERE grade = 'A';`\n",
    "\n",
    "to select all the columns and the rows containing information about students who got As. Another common database task is aggregation. That is, reducing your data by breaking it into chunks and summarizing each chunk.\n",
    "\n",
    "This is done in SQL using the `GROUP BY` command. This command breaks your data into groups and applies a function from your `SELECT` statement to each group. For example, if you wanted to count the number of athletes from each country, you could use the query\n",
    "\n",
    "`SELECT COUNT(*) FROM athletesDF GROUP BY country;`\n",
    "\n",
    "`GROUP BY` origin tells SQL that you want the output to have a row for each unique value of the country column. The `SELECT` statement selects the values you want to populate each of the columns. Here, we want to `COUNT()` every row in each of the groups.\n",
    "\n",
    "It's possible to `GROUP BY` more than one column. When you do this, the resulting table has a row for every combination of the unique values in each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----------+-------+--------+\n",
      "|result_id|medal|athlete_id|game_id|event_id|\n",
      "+---------+-----+----------+-------+--------+\n",
      "|        1|   NA|         1|     39|       1|\n",
      "|        2|   NA|         2|     49|       2|\n",
      "|        3|   NA|         3|      7|       3|\n",
      "|        4| Gold|         4|      2|       4|\n",
      "|        5|   NA|         5|     36|       5|\n",
      "+---------+-----+----------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF.registerTempTable(\"result\")\n",
    "sqlContext.sql(\"SELECT * FROM result\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Pandafy a Spark DataFrame\n",
    "\n",
    "Suppose you've run a query on your huge dataset and aggregated it down to something a little more manageable.\n",
    "\n",
    "Sometimes it makes sense to then take that table and work with it locally using a tool like `pandas`. Spark `DataFrames` make that easy with the `.toPandas()` method. Calling this method on a Spark `DataFrame` returns the corresponding pandas `DataFrame`. It's as simple as that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|sport_id|        sport|\n",
      "+--------+-------------+\n",
      "|       1|   Basketball|\n",
      "|       2|         Judo|\n",
      "|       3|     Football|\n",
      "|       4|   Tug-Of-War|\n",
      "|       5|Speed Skating|\n",
      "+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sportDF.registerTempTable(\"sport\")\n",
    "sqlContext.sql(\"SELECT * FROM sport\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sport_id</th>\n",
       "      <th>sport</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Basketball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Judo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Football</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Tug-Of-War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Speed Skating</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sport_id          sport\n",
       "0         1     Basketball\n",
       "1         2           Judo\n",
       "2         3       Football\n",
       "3         4     Tug-Of-War\n",
       "4         5  Speed Skating"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"SELECT * FROM sport\"\n",
    "# Run the query\n",
    "pd_sports = spark.sql(query)\n",
    "# Convert the results to a pandas DataFrame\n",
    "pd_sports = pd_sports.toPandas()\n",
    "# Print the head of pd_counts\n",
    "pd_sports.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on you can use the regular functions you know from `pandas`! \n",
    "\n",
    "--- \n",
    "\n",
    "## 16. Put some Spark in your data\n",
    "\n",
    "We already saw how to move data from Spark to `pandas`. However, maybe you want to go the other direction, and put a `pandas` DataFrame into a Spark cluster! The `SparkSession` class has a method for this as well.\n",
    "\n",
    "The `.createDataFrame()` method takes a `pandas` DataFrame and returns a Spark DataFrame.\n",
    "\n",
    "The output of this method is stored locally, not in the `SparkSession` catalog. This means that you can use all the Spark DataFrame methods on it, but you can't access the data in other contexts.\n",
    "\n",
    "For example, a SQL query (using the `.sql()` method) that references your DataFrame will throw an error. To access the data in this way, you have to save it as a temporary table. (as we did we the tables above).\n",
    "\n",
    "You can do this using the `.createTempView()` Spark DataFrame method, which takes as its only argument the name of the temporary table you'd like to register. This method registers the DataFrame as a table in the catalog, but as this table is temporary, it can only be accessed from the specific `SparkSession` used to create the Spark DataFrame.\n",
    "\n",
    "There is also the method `.createOrReplaceTempView()`. This safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. You can use this method to avoid running into problems with duplicate tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = spark.createDataFrame(pd_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Viewing tables\n",
    "\n",
    "Your `SparkSession` has an attribute called `catalog` which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\n",
    "\n",
    "One of the most useful is the `.listTables()` method, which returns the names of all the tables in your cluster as a list.\n",
    "\n",
    "Remark that the output of the `sqlContext.read.schema` method is stored locally, not in the `SparkSession` catalog. This means that you can use all the Spark DataFrame methods on it, but you can't access the data in other contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='country', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='result', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='sport', database=None, description=None, tableType='TEMPORARY', isTemporary=True), Table(name='temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop() # close the spark session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
